{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5ZlL0fO_Nme"
   },
   "source": [
    "# Milestone 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnX1dhtN_TUh"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Differentiation is ubiquitous in almost all aspects of computer science, mathematics, and physics. It is used for numeric root-finding as in Newton's Method, and used for optimization with different forms of gradient descent.\n",
    "However, calculating analytic derivatives is difficult and can lead to exponentially growing abstract syntax trees, which makes finding the derivative infeasible in many cases. Calculating the derivative numerically using the limit definition runs into numeric instabilities due to limited machine precision. Automatic differentiation addresses both of these issues - it uses the chain rule and the fact that computers calculate any function as a sequence of elementary operations to find the derivative.\n",
    "\n",
    "The package can also evaluate the Hessian of multivariable functions and higher order derivatives of scalar functions. This is particularly useful for optimization problems, where convergence on a function minimum is often significantly faster when Hessian information is included in the root finding algorithm. This is useful for Control Theory applications, as well as many important algorithms used in robotics. Calculating the Hessian is useful for path planning, as well as for inverse kinematics. Since it is relatively difficult to get a closed form solution for the Hessian in complicated inverse kinematics problems, it is useful to be able to use Automatic Differentiation for the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2L2ci5Tw_ZJF"
   },
   "source": [
    "## Installation and Usage\n",
    "\n",
    "\n",
    "### Installing from Source\n",
    "\n",
    "If you want the latest nightly version of AutoDiffX, clone from our github\n",
    "repository and install the latest version directly.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/CS207-Project-Team-1/cs207-FinalProject autodiffx\n",
    "cd autodiffx\n",
    "pip install -r requirements.txt\n",
    "python3 setup.py install\n",
    "```\n",
    "\n",
    "If you are working on a python virtual environment or Mac OSX or your user's\n",
    "python distribution, this should work. If editing the system python, you may\n",
    "need to run the last command with root permissions by adding `sudo`.\n",
    "\n",
    "### Installing from pip\n",
    "\n",
    "For the stable version, you can install our package from PyPI.\n",
    "\n",
    "```bash\n",
    "pip install autodiffx\n",
    "```\n",
    "\n",
    "## Testing\n",
    "\n",
    "All of the tests are run using pytest. To run pytest, you want to be in the\n",
    "root directory of the repository. To ensure that `pytest` gets the imports\n",
    "correct, you want to run it such that it adds the current path to `PYTHONPATH`.\n",
    "The easiest way to do so is:\n",
    "\n",
    "```bash\n",
    "python -m pytest\n",
    "```\n",
    "\n",
    "This should run all of the tests for the package.\n",
    "\n",
    "Currently, our only module that we actually use lives in the `ad/` folder. The tests can be found in the different test files in `ad/tests`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSxx1_h9_UTc"
   },
   "source": [
    "## Background\n",
    "\n",
    "**Automatic Differentiation**\n",
    "\n",
    "Automatic differentiation relies heavily on the principles of chain rule differentiation. A graph of elementary functions is built to calculate the values of more complex functions. Using the chain rule on the graph of elementary functions, the value of the derivative at each node can also be calculated. This gives us the ability to calculate the values of functions and their derivatives, no matter how complex, to near machine precision (a significant advantage compared to alternatives such as finite differences). \n",
    "\n",
    "The chain rule tells us that:\n",
    "\\begin{align}\n",
    "\\frac{d}{dx}(f(g(x))) &= f'(g(x))g'(x)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Since each step in our graph is just a combination of linear operations, we can find the derivative at a node by considering the value and derivative of the expressions at the previous node. By starting with an initial 'seed' vector for the derivative (often set to 1), we can find the derivative in any desired search direction. \n",
    "\n",
    "Below is an example of constructing a graph to find the exact values of a function and its derivative. The function we used was:\n",
    "\n",
    "$$f\\left(x, y, z\\right) = \\dfrac{1}{xyz} + \\sin\\left(\\dfrac{1}{x} + \\dfrac{1}{y} + \\dfrac{1}{z}\\right)$$\n",
    "\n",
    "We worked through this, starting with trace elements $x_1$ for $x$,  $x_2$ for $y$ and  $x_3$ for $z$. We wanted to solve this function at $(x, y, z) = (1, 2, 3)$.\n",
    "\n",
    "| Trace | Elementary Function | Current Value | Elementary Function Derivative | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  | $\\nabla_{z}$ Value  |\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------: | :-----------------: | :-----------------: |\n",
    "| $x_{1}$ | $x$ | 1 | $\\dot{x}$ | 1 | 0 | 0 | \n",
    "| $x_{2}$ | $y$ | 2 | $\\dot{y}$ | 0 | 1 | 0 | \n",
    "| $x_{3}$ | $z$ | 3 | $\\dot{z}$ | 0 | 0 | 1 | \n",
    "| $x_{4}$ | $1/x_{1}$ | 1 | $-\\dot{x}_{1}/x_{1}^{2}$ | $-1$ | $0$ | $0$ | \n",
    "| $x_{5}$ | $1/x_{2}$ | 1/2 | $-\\dot{x}_{2}/x_{2}^{2}$ | $0$ | $-1/4$ | $0$ | \n",
    "| $x_{6}$ | $1/x_{3}$ | 1/3 |  $-\\dot{x}_{3}/x_{3}^{2}$ | $0$ | $0$ | $-1/9$ | \n",
    "| $x_{7}$ | $x_{4} + x_{5} + x_{6}$ | 11/6 |$\\dot{x}_{4} + \\dot{x}_{5} + \\dot{x}_{6}$ | -1 | -0.25 | -0.11 |\n",
    "| $x_{8}$ | $sin(x_{7})$ | 0.966 |$\\dot{x}_{7}cos(x_{7})$ | 0.260 | 0.065 | 0.029 | \n",
    "| $x_{9}$ | $x_{4}x_{5}x_{6}$| 1/6 |$\\dot{x}_{4}x_{5}x_{6} + \\dot{x}_{5}x_{4}x_{6} + \\dot{x}_{6}x_{4}x_{5} $ |-0.167 | -0.083  | -0.056 | \n",
    "| $x_{10}$ | $x_{8} + x_{9}$ | 1.132 |$\\dot{x}_{8} + \\dot{x}_{9}$ | 0.093| -0.018  | -0.027 | \n",
    "\n",
    "This isn't a very complicated function, but it shows how we can use the most basic of functions to create a graph allowing us to find exact values and gradients.\n",
    "\n",
    "**Higher-Order Derivatives**\n",
    "\n",
    "An effective approach to high-order automatic differention can be obtained by considering the calculus of a Taylor series. If we define $f_k$ as follows:\n",
    "\n",
    "$$f_k = f_{k}(x_0) = \\dfrac{f^{(k)}(x_0)}{k!}$$\n",
    "\n",
    "We can show that basic arithmetic operations are as follow, where $f$ and $g$ are separate functions with the same input variable. Full derivations of these basic operations can be found [here](https://www.sintef.no/globalassets/project/evitameeting/2010/ad2010.pdf):\n",
    "\n",
    "$$(f + g)_k = f_k + g_k$$\n",
    "\n",
    "$$(f - g)_k = f_k - g_k$$\n",
    "\n",
    "$$(f \\times g)_k = \\sum_{i = 0}^{k} f_{i}g_{k-i} $$\n",
    "\n",
    "$$(f \\div g)_k = \\dfrac{1}{g_0} \\left(f_k - \\sum_{i = 0}^{k - 1} (f \\div g)_{i} g_{k-i}\\right) $$\n",
    "\n",
    "$$ (e^g)_k = \\dfrac{1}{k} \\sum_{i = 1}^{k} ig_{i}(e^g)_{k-i}$$\n",
    "\n",
    "**Hessian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z13vUmhoQFdB"
   },
   "source": [
    "## Simple Usage\n",
    "\n",
    "Our package supports scalar functions with scalar or multivariable inputs. \n",
    "\n",
    "**Scalar Input**\n",
    "\n",
    "Suppose that we wanted to find the derivative of the following function, $f(x)$:\n",
    "\n",
    "$$f(x) = x \\exp(\\cos(x) + x^2)$$\n",
    "\n",
    "First, we have to allocate a Variable for our input variable $x$. We can do that, and give it a string identifier `\"x\"` for convenience later. \n",
    "\n",
    "```python\n",
    "import ad\n",
    "x = ad.Variable('x')\n",
    "```\n",
    "Now, we have to define our function. Our module `ad` has a lot of built in functions, and all of the regular operators `+`, `-`, `*`, `/`, `**` should also work. We can make our function $f(x)$ by just writing it out in code.\n",
    "\n",
    "```python\n",
    "f = x * ad.Exp(ad.Cos(x) + x ** 2)\n",
    "```\n",
    "\n",
    "Now, we've defined our function in terms of our input variable $x$. Now, in order to evaluate our function or evaluate the derivative at a specific point we actually have to provide a value for the input. Since we later plan on handling multiple variable inputs, we pass the input in as a dictionary. To evaluate the function, we use the function ```eval```. For the derivative at a point, we use ```d```. Suppose that we wanted to evaluate the function $f$ and its derivative at $x = 0$ and $x = 1$. We can just run:\n",
    "\n",
    "```python\n",
    ">>> f.eval({x: 0})\n",
    "2.718281828459045\n",
    "\n",
    ">>> f.d({x: 0})\n",
    "1.0\n",
    "\n",
    ">>> f.eval({x: 1})\n",
    "5.666000617166735\n",
    "\n",
    ">>> f.d({x: 1})\n",
    "6.405697099891925\n",
    "```\n",
    "\n",
    "It is also possible to evaluate higher order derivatives of functions with scalar inputs. This can be done using ```f.d_n(n, val)``` where n and val are arguments expressing the desired order of differentiation and the value at which the derivative should be evaluated. We can run:\n",
    "\n",
    "```python\n",
    ">>> f.d_n(n = 3, val = 2)\n",
    "1902.7256925837773\n",
    "```\n",
    "\n",
    "**Multi-Variable Input**\n",
    "\n",
    "Suppose that we wanted to find the derivative of the following function, $f(x, y)$:\n",
    "\n",
    "$$f(x) = x \\exp(\\cos(y) + x^2)$$\n",
    "\n",
    "We have to allocate a variables for our inputs $x$ and $y$. We can do that, and give them string identifiers `\"x\"` and `\"y\"` for convenience later. \n",
    "\n",
    "```python\n",
    "import ad\n",
    "x = ad.Variable('x')\n",
    "y = ad.Variable('y')\n",
    "```\n",
    "We now define our function.\n",
    "\n",
    "```python\n",
    "f = x * ad.Exp(ad.Cos(y) + x ** 2)\n",
    "```\n",
    "\n",
    "We can evaluate our function and it's derivative exactly as we evaluated our scalar input example earlier. Suppose that we wanted to evaluate the function $f$ and its derivative at $x = 0$ and $y = 1$. We can just run:\n",
    "\n",
    "```python\n",
    ">>> f.eval({x: 0, y : 1})\n",
    "1.7165256995489035\n",
    "\n",
    ">>> f.d({x: 0, y : 1})\n",
    "{y: -1.4444065708474794, x: 1.0}\n",
    "```\n",
    "\n",
    "Using the Hessian is very similar to calling the derivative of a multivariable function. If the function depends on $n$ input variables, the package will return an $n \\times n$ matrix in the form of a dictionary of dictionaries, where each sub-dictionary refers to one of the rows in the matrix. \n",
    "\n",
    "Hence, for our multivariable example above, $f(x, y)$:\n",
    "\n",
    "$$f(x) = x \\exp(\\cos(y) + x^2)$$\n",
    "\n",
    "We can evaluate the Hessian by calling:\n",
    "\n",
    "```python\n",
    ">>> f.hessian({x: 0, y : 1})\n",
    "{y: {y: 0.28798342608583105, x: 0.0}, x: {y: 0.0, x: 3.433051399097807}}\n",
    "```\n",
    "If we want the specific second order derivatives, we can evaluate specific elements from the Hessian by calling the keys. For example, if we wanted to find the second order derivative of our function with respect to $x$, we would call:\n",
    "\n",
    "```python\n",
    ">>> f.hessian({x: 0, y : 1})[x][x]\n",
    " 3.433051399097807\n",
    "```\n",
    "\n",
    "More complicated demonstrations (scalar input Newton's Method, multivariable input Newton's Method with Hessian) can be found in Jupyter notebooks at the top level directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cfu_jYk_iK2"
   },
   "source": [
    "## Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vF3Ni2RGPJTt"
   },
   "source": [
    "## Directory structure\n",
    "\n",
    "```bash\n",
    "cs207project\n",
    "├── LICENSE\n",
    "├── README\n",
    "├── docs\n",
    "│   ├── Milestone1.ipynb\n",
    "│   ├── Milestone2.ipynb\n",
    "│   ├── Milestone3.ipynb \n",
    "│   └── SETUP.md\n",
    "├── ad\n",
    "│   ├── __init__.py\n",
    "│   ├── ad.py\n",
    "│   ├── simple_ops.py\n",
    "│   ├── activation_ops.py.../\n",
    "│   └── tests\n",
    "│       ├── test_complex_ops.py\n",
    "│       ├── test_d_expr.py\n",
    "│       ├── test_expression.py\n",
    "│       ├── test_high_order.py\n",
    "│       ├── test_multivar_hessian.py\n",
    "│       ├── test_simple_hessian.py\n",
    "│       ├── test_simple_ops.py\n",
    "│       └── test_vector.py\n",
    "├── requirements.txt\n",
    "├── Newton_Method_Demonstration.ipynb\n",
    "├── Hessian_Demonstration.ipynb\n",
    "└── setup.py\n",
    "```\n",
    "\n",
    "## Modules\n",
    "\n",
    "`ad`: Implementation of core structures used in our graph-based automatic differentiation library. +, -, *, /, and exponentiation is also implemented here to support simple operator overloading.\n",
    "\n",
    "`simple_ops`: Unary operations including Sin(x), Cos(x), Tan(x), Sinh(x), Cosh(x), Tanh(x), Exp(x), Log(x), Arcsin(x), Arccos(x), Arctan(x), Logistic(x).\n",
    "\n",
    "## Test\n",
    "\n",
    "See Testing.\n",
    "\n",
    "## Installment\n",
    "\n",
    "See Installation and Usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUlwF_YJ_mBU"
   },
   "source": [
    "## Implementation Details\n",
    "\n",
    "We implemented our automatic differentiation using a graph based structure. First, the user will build up their function in terms of elementary operations. Then, the user will be able to feed a value for their input variables, and the package will calculate the derivatives at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcrJbb9E_4ZC"
   },
   "source": [
    "### Automatic Differentiation Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvtimCjqAAdL"
   },
   "source": [
    "Our core data structure is the computational graph. Every node in the computational graph would be an \"Expression\", which is our core class. \n",
    "\n",
    "Based on \"Expression\", we have \"Variable(Expression)\", \"Constant(Expression)\", \"Unop(Expression)\" and \"Biop(Expression)\". \"Unop\" is for unary operations such as log and power. \"Biop\" is for binary operations such as addition. For \"Expression\" and its subclasses, there are two important attributes: \"grad\", a boolean variable indicating whether we want to calculate the derivative or not; children, a list of nodes pointing to current one, the number of children is one for \"Unop\" and the number of chilren for \"Biop\" is two.\n",
    "\n",
    "The elementary functions we support are:\n",
    "\n",
    "Unary operations: \n",
    "\n",
    "* Sin(x)\n",
    "* Cos(x)\n",
    "* Tan(x)\n",
    "* Sinh(x)\n",
    "* Cosh(x)\n",
    "* Tanh(x)\n",
    "* Exp(x)\n",
    "* Log(x)\n",
    "* Arcsin(x)\n",
    "* Arccos(x)\n",
    "* Arctan(x)\n",
    "* Logistic(x)\n",
    "\n",
    "Binary operations: \n",
    "\n",
    "* Addition (+)\n",
    "* Substraction (-)\n",
    "* Multiplication (*)\n",
    "* Division (/)\n",
    "* Power(x, n)\n",
    "\n",
    "We will mainly be using `numpy` as an external dependency for mathematical and vector operations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPdt0eOf_7xP"
   },
   "source": [
    "### Multivariate Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6K5gmNpFFhL"
   },
   "source": [
    "We want to also support getting the Jacobian when we have more than one input for a scalar function. We are able to support functions of the form\n",
    "$$f: \\mathbb{R}^m \\to \\mathbb{R}$$\n",
    "Our implementation for multivariate inputs is by using a `dict` to hold the different partial derivatives. We can go through an example. Suppose that a user wanted the Jacobian of a function:\n",
    "\n",
    "$$f(x, y, z) = x  + y \\cos(yz)$$\n",
    "\n",
    "Then, we would be able to find the Jacobian in code using the following syntax.\n",
    "\n",
    "```python\n",
    ">>> x = ad.Variable('x')\n",
    ">>> y = ad.Variable('y')\n",
    ">>> z = ad.Variable('z')\n",
    "\n",
    ">>> f = x + y * ad.Cos(y * z)\n",
    "\n",
    ">>> f.eval({x:6, y:1, z:0})\n",
    "7\n",
    ">>> f.d({x:6, y:1, z:0})\n",
    "{x: 1, y: 1, z: 0}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Feature - Higher Order Derivatives and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pZ4XasAJm8_"
   },
   "source": [
    "**Higher Order Derivatives**\n",
    "\n",
    "See background.\n",
    "\n",
    "**Hessian**\n",
    "\n",
    "Our package implements the Hessian for functions of the form\n",
    "$$f: \\mathbb{R}^m \\to \\mathbb{R}$$\n",
    "In this case, the Hessian should be a square matrix. Since the Hessian will be a square matrix with entries of the form\n",
    "$$\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$\n",
    "for some $i, j$, we return a dictionary of dictionaries for the Hessian. This makes it easy to index the Hessian as ```hessian[x][y]``` to get the second order derivative with respect to x and y. An example function and calculated Hessian will be look something similar to the follows.\n",
    "\n",
    "$$f: \\mathbb{R}^3 \\to \\mathbb{R}$$\n",
    "$$f(x, y, z) = xy + z$$\n",
    "\n",
    "Then, the code uses the function `hessian()` to calculate the Hessian at a certain point.\n",
    "\n",
    "```python\n",
    ">>> x = ad.Variable('x')\n",
    ">>> y = ad.Variable('y')\n",
    ">>> z = ad.Variable('z')\n",
    "\n",
    ">>> f = x * y + z\n",
    "\n",
    ">>> f.eval({x:1, y:1, z:1})\n",
    "2\n",
    "\n",
    ">>> f.hessian({x:1, y:1, z:1})\n",
    "{x: {x: 0, y: 1, z:} y: {x: 1, y: 0, z: 0} z: {x: 0, y: 0, z: 0}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariate Outputs**\n",
    "\n",
    "Currently, our package only fully supports scalar functions with scalar or multivariate input.\n",
    "\n",
    "However, the next step after supporting multivariate inputs is to also support multivariate outputs. We will create a wrapper class `ad.Vector` that essentially wraps multiple different `ad.Expression` instances together. This will allow us to combine multiple scalar functions into a multivariate function. For example, suppose that a user wanted to find the Jacobian of a function in the form:\n",
    "$$f: \\mathbb{R}^3 \\to \\mathbb{R}^3$$\n",
    "and suppose that the function was in the form:\n",
    "$$f(x, y, z) = \n",
    "\\begin{pmatrix}\n",
    "x + y + z \\\\\n",
    "\\cos(yz) \\\\\n",
    "\\exp(x - y - z)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be represented in code as follows:\n",
    "\n",
    "```python\n",
    ">>> x = ad.Variable('x')\n",
    ">>> y = ad.Variable('y')\n",
    ">>> z = ad.Variable('z')\n",
    "\n",
    ">>> f1 = x + y + z\n",
    ">>> f2 = ad.Cos(y * z)\n",
    ">>> f3 = ad.Exp(x - y - z)\n",
    "\n",
    ">>> f = ad.Vector(f1, f2, f3)\n",
    ">>> f.eval({x:0, y:0, z:0})\n",
    "[0, 1, 1]\n",
    "\n",
    ">>> f.d({x:0, y:0, z:0})\n",
    "{x: [1, 0, 1], y: [1, 0, -1], z: [1, 0, -1]}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Milestone2",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
